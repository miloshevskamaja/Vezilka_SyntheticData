# -*- coding: utf-8 -*-
"""Template-Based & KG & KGAST Synthetic Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tN-Kab5guGsDSZhM_Rvkjybs2B-Z9gf9
"""

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/Vezilka/SpisokOpstiniMK.csv", encoding="utf-8", skiprows=3)
df.head()

df = df.iloc[:, :-1]

df.head()

df.columns = [
    "opstina_id",
    "opstina",
    "sifra_nas_mesto",
    "naseleno_mesto",
    "tip_naseleno_mesto"
]

print(df.columns)
print(df.shape)
df.isna().sum()

df = df.dropna(subset=["naseleno_mesto", "opstina", "tip_naseleno_mesto"])
df.shape

df["tip_naseleno_mesto"] = (
    df["tip_naseleno_mesto"]
    .astype(str)
    .str.strip()
    .replace({"г": "град", "с": "село"})
)

df.info()

!pip -q install "argilla>=2.0.0" pandas tqdm huggingface_hub openai

import argilla as rg
from google.colab import userdata

ARGILLA_API_URL = (userdata.get("ARGILLA_API_URL") or "").strip()
ARGILLA_API_KEY = (userdata.get("ARGILLA_API_KEY") or "").strip()

print("URL repr:", repr(ARGILLA_API_URL))
print("KEY starts with:", ARGILLA_API_KEY[:10])

client = rg.Argilla(api_url=ARGILLA_API_URL, api_key=ARGILLA_API_KEY)
print(client.me)

required_cols = ["opstina_id", "opstina",  "sifra_nas_mesto", "naseleno_mesto","tip_naseleno_mesto"]
missing = [c for c in required_cols if c not in df.columns]
if missing:
    raise ValueError(f"Missing columns in df: {missing}")

df2 = df.copy()

df2["sifra_nas_mesto"] = df2["sifra_nas_mesto"].astype("Int64").astype(str)

for c in ["naseleno_mesto","opstina"]:
    df2[c] = df2[c].astype(str).str.strip()

df2 = df2.dropna(subset=["naseleno_mesto","opstina","tip_naseleno_mesto"]).reset_index(drop=True)
df2.shape, df2.head(3)

"""Part 1 - with LLM MistralAi - bad data with hallucination and confusion between Macedonian, Bulgarian and Serbian language  """

def make_prompt(row, n_sentences=2):
    return (
        f"Генерирај {n_sentences} кратки, различни реченици на македонски јазик, "
        f"строго базирани на следниве факти (не додавај ништо друго):\n"
        f"- Населено место: {row['naseleno_mesto']}\n"
        f"- Тип: {row['tip_naseleno_mesto']}\n"
        f"- Општина: {row['opstina']}\n"
        "Правила:\n"
        "1) Не измислувај популација, координати, региони, историја.\n"
        "2) Не користи англиски.\n"
        "3) Врати само речениците (секојa во нов ред).\n"
        "4) Ако некој податок недостига, немој да претпоставуваш – користи само даденото.\n"
    )

from google.colab import userdata

from huggingface_hub import InferenceClient

HF_TOKEN = userdata.get("HF_TOKEN")

client_hf = InferenceClient(
    model="mistralai/Mistral-7B-Instruct-v0.2",
    token=HF_TOKEN
)

def gen_hf_chat(prompt, temperature=0.6, max_tokens=220):
    resp = client_hf.chat.completions.create(
        messages=[
            {"role": "system", "content": "Пишувај само на македонски (кирилица). Биди строго фактолошки."},
            {"role": "user", "content": prompt},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return resp.choices[0].message.content.strip()

from tqdm import tqdm

N = 200
sample_df = df2.sample(min(N, len(df2)), random_state=42).reset_index(drop=True)

prompts, generations = [], []

for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc="Generating (HF chat)"):
    prompt = make_prompt(row, n_sentences=2)
    prompts.append(prompt)

    try:
        text = gen_hf_chat(prompt)
    except Exception as e:
        text = f"[ERROR] {type(e).__name__}: {e}"

    generations.append(text)

sample_df["prompt"] = prompts
sample_df["generation"] = generations

generations



"""Part 2 - Template Based Generation"""

import random

TEMPLATES = [
    lambda r: [
        f"„{r['naseleno_mesto']}“ е {r['tip_naseleno_mesto']} во општина „{r['opstina']}“.",
        f"Населеното место „{r['naseleno_mesto']}“ припаѓа на општина „{r['opstina']}“."
    ],
    lambda r: [
        f"{r['tip_naseleno_mesto'].capitalize()}то „{r['naseleno_mesto']}“ се наоѓа во општина „{r['opstina']}“.",
        f"Општина „{r['opstina']}“ го опфаќа населеното место „{r['naseleno_mesto']}“."
    ],
    lambda r: [
        f"„{r['naseleno_mesto']}“ е населено место од тип {r['tip_naseleno_mesto']} во општина „{r['opstina']}“.",
        f"Како {r['tip_naseleno_mesto']}, „{r['naseleno_mesto']}“ административно припаѓа на општина „{r['opstina']}“."
    ],
    lambda r: [
        f"Во општина „{r['opstina']}“ се наоѓа {r['tip_naseleno_mesto']}то „{r['naseleno_mesto']}“.",
        f"„{r['naseleno_mesto']}“ е {r['tip_naseleno_mesto']} кое е дел од општина „{r['opstina']}“."
    ],
]

def gen_template_var(row, seed=None):
    if seed is not None:
        random.seed(seed)
    tmpl = random.choice(TEMPLATES)
    lines = tmpl(row)
    return "\n".join(lines)

from tqdm import tqdm
import pandas as pd

N = len(df2)
sample_df = df2.sample(N, random_state=42).reset_index(drop=True)

generations = []

for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc="Template-based generation"):
    text = gen_template_var(row)
    generations.append(text)

sample_df["generation"] = generations

sample_df[["naseleno_mesto", "opstina", "tip_naseleno_mesto", "generation"]].head(10)

generations

import json

def make_sft_prompt(row):
    return (
        f"Населено место: {row['naseleno_mesto']}\n"
        f"Тип: {row['tip_naseleno_mesto']}\n"
        f"Општина: {row['opstina']}\n"
        f"Напиши две фактолошки реченици на македонски."
    )

records = []
for _, row in sample_df.iterrows():
    records.append({
        "prompt": make_sft_prompt(row),
        "completion": row["generation"]
    })

with open("mk_settlements_template_sft.jsonl", "w", encoding="utf-8") as f:
    for r in records:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

print("Saved: mk_settlements_template_sft.jsonl")

"""Part 3 - Generate synthetic data using KGs"""

import pandas as pd

def build_triples(df):
    triples = []
    for _, r in df.iterrows():
        nm = r["naseleno_mesto"]
        op = r["opstina"]
        typ = r["tip_naseleno_mesto"]

        triples.append((nm, "е_тип", typ))
        triples.append((nm, "припаѓа_на", op))
        triples.append((op, "има_населено_место", nm))
    return pd.DataFrame(triples, columns=["subj", "pred", "obj"])

triples_df = build_triples(df2)
triples_df.head(10), triples_df.shape

from collections import defaultdict

# општина -> листа на населени места
op_to_nm = defaultdict(list)

# населено место -> општина
nm_to_op = {}

# населено место -> тип
nm_to_type = {}

for _, r in df2.iterrows():
    nm = r["naseleno_mesto"]
    op = r["opstina"]
    typ = r["tip_naseleno_mesto"]
    op_to_nm[op].append(nm)
    nm_to_op[nm] = op
    nm_to_type[nm] = typ

import random

def kg_text_for_row(row, seed=None):
    if seed is not None:
        random.seed(seed)

    nm = row["naseleno_mesto"]
    op = row["opstina"]
    typ = row["tip_naseleno_mesto"]

    neighbors = [x for x in op_to_nm[op] if x != nm]
    other = random.choice(neighbors) if neighbors else None
    other_typ = nm_to_type.get(other) if other else None

    candidates = []

    candidates.append(
        "\n".join([
            f"„{nm}“ е {typ} во општина „{op}“.",
            f"Општина „{op}“ го опфаќа населеното место „{nm}“."
        ])
    )

    candidates.append(
        "\n".join([
            f"Во општина „{op}“ се наоѓа {typ}то „{nm}“.",
            f"Населеното место „{nm}“ административно припаѓа на општина „{op}“."
        ])
    )

    if other:
        candidates.append(
            "\n".join([
                f"„{nm}“ е {typ} во општина „{op}“.",
                f"Во истата општина се наоѓа и {other_typ}то „{other}“."
            ])
        )
        candidates.append(
            "\n".join([
                f"Општина „{op}“ ги опфаќа населените места „{nm}“ и „{other}“.",
                f"„{nm}“ е {typ}, а „{other}“ е {other_typ}."
            ])
        )

    if other:
        candidates.append(
            "\n".join([
                f"Во општина „{op}“ се наоѓаат „{nm}“ и „{other}“.",
                f"„{nm}“ е {typ}, додека „{other}“ е {other_typ}."
            ])
        )

    return random.choice(candidates)

from tqdm import tqdm

kg_generations = []
for _, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc="KG-based generation"):
    kg_generations.append(kg_text_for_row(row))

sample_df["kg_generation"] = kg_generations

sample_df[["naseleno_mesto","opstina","tip_naseleno_mesto","generation","kg_generation"]].head(10)

kg_generations

rows = []
for _, r in sample_df.iterrows():
    base = {
        "opstina_id": r["opstina_id"],
        "sifra_nas_mesto": r["sifra_nas_mesto"],
        "naseleno_mesto": r["naseleno_mesto"],
        "opstina": r["opstina"],
        "tip_naseleno_mesto": r["tip_naseleno_mesto"],
    }
    rows.append({**base, "source": "template", "text": r["generation"]})
    rows.append({**base, "source": "kg", "text": r["kg_generation"]})

train_df = pd.DataFrame(rows)

train_df.shape

train_df.head(5)

train_df['text'][15]

import json

def make_prompt_from_row(r):
    return (
        f"Населено место: {r['naseleno_mesto']}\n"
        f"Тип: {r['tip_naseleno_mesto']}\n"
        f"Општина: {r['opstina']}\n"
        f"Напиши две фактолошки реченици на македонски."
    )

out_path = "synthetic_sententes_kg.jsonl"
with open(out_path, "w", encoding="utf-8") as f:
    for _, r in train_df.iterrows():
        rec = {
            "prompt": make_prompt_from_row(r),
            "completion": r["text"],
            "meta": {"source": r["source"]}
        }
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

print("Saved:", out_path)

"""Part 4 - KGAST pipeline"""

!pip -q install sentence-transformers faiss-cpu rapidfuzz

import json, random, re, math
import numpy as np
import pandas as pd
from tqdm import tqdm
from rapidfuzz import fuzz, process
from sentence_transformers import SentenceTransformer
import faiss

CONFIG = {
    "kshots": 4,
    "num_candidates": 3,
    "vote_runs": 5,
    "annot_runs": 5,
    "annot_threshold": 0.5,

    "min_triple_coverage": 1.0,
    "max_unknown_entity_ratio": 0.20,
    "entity_match_threshold": 90,
    "triple_match_threshold": 88,

    "max_regen_rounds": 3,
    "gen_temperature": 0.8,
    "gen_max_new_tokens": 220,

    "vote_temperature": 0.3,
    "vote_max_new_tokens": 5,

    "json_temperature": 0.2,
    "json_max_new_tokens": 420,
}

RELATIONS = ["е_тип", "припаѓа_на", "има_населено_место"]

def normalize_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s

def norm_key(s: str) -> str:
    s = normalize_text(s).lower()
    s = re.sub(r"[\"'“”‘’\(\)\[\]\{\}\,\.\:\;\!\?\-–—/\\]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize_simple(s: str):
    s = norm_key(s)
    return [t for t in s.split(" ") if t]

def fuzzy_in_text(entity: str, text: str, threshold: int = 90) -> bool:
    """
    Robust presence check: fuzzy partial ratio over normalized forms.
    """
    e = norm_key(entity)
    t = norm_key(text)
    if not e or not t:
        return False

    if e in t:
        return True

    return fuzz.partial_ratio(e, t) >= threshold

def best_fuzzy_match(query: str, choices: list[str], threshold: int = 90):
    if not choices:
        return None, 0
    match = process.extractOne(query, choices, scorer=fuzz.WRatio)
    if not match:
        return None, 0
    cand, score, _ = match
    if score >= threshold:
        return cand, score
    return None, score

#Building richer kg triples per row
"""
KG triples from structured table. Keep it factual.
Adds optional neighbor facts within same municipality for mild diversity.
"""


def build_row_kg(row, seed=None):
    if seed is not None:
        random.seed(seed)

    nm  = normalize_text(row["naseleno_mesto"])
    op  = normalize_text(row["opstina"])
    typ = normalize_text(row["tip_naseleno_mesto"])

    nm_type  = "LOC"
    op_type  = "LOC"
    typ_type = "MISC"

    triples = [
        (nm, "е_тип", typ, nm_type, typ_type),
        (nm, "припаѓа_на", op, nm_type, op_type),
        (op, "има_населено_место", nm, op_type, nm_type),
    ]

    neighbors = [x for x in op_to_nm.get(op, []) if normalize_text(x) != nm]
    if neighbors:
        other = normalize_text(random.choice(neighbors))
        other_typ = normalize_text(nm_to_type.get(other, ""))
        triples += [
            (op, "има_населено_место", other, op_type, nm_type),
            (other, "припаѓа_на", op, nm_type, op_type),
        ]
        if other_typ:
            triples.append((other, "е_тип", other_typ, nm_type, "MISC"))

    return triples

#Natural-language triple format (KGAST-style representation).

def kg_to_nl_triples(triples):
    out = []
    for h, r, t, ht, tt in triples:
        out.append(f'("{h}":{ht}, "{r}", "{t}":{tt})')
    return "\n".join(out)

# Better for embedding retrieval than raw symbol-heavy triples.
def kg_signature(triples):
    ents = []
    rels = []
    for h, r, t, ht, tt in triples:
        ents.append(f"{h}({ht})")
        ents.append(f"{t}({tt})")
        rels.append(r)
    ents = list(dict.fromkeys(ents))
    rels = list(dict.fromkeys(rels))
    return " | ".join([
        "ENTITIES: " + ", ".join(ents),
        "RELATIONS: " + ", ".join(rels),
    ])

def get_kg_entities(triples):
    ents = []
    for h, r, t, ht, tt in triples:
        ents.append(h)
        ents.append(t)
    return list(dict.fromkeys([normalize_text(x) for x in ents if normalize_text(x)]))

def make_example_bank(df_like):
    bank = []
    for _, r in df_like.iterrows():
        triples = build_row_kg(r, seed=42)
        bank.append({
            "text": normalize_text(r["text"]),
            "kg_triples": triples,
            "kg_str": kg_to_nl_triples(triples),
            "sig": kg_signature(triples),
        })
    return bank

example_bank = make_example_bank(train_df)

def fallback_example_bank_from_table(df_like, n=500, seed=42):
    rnd = df_like.sample(n=min(n, len(df_like)), random_state=seed).reset_index(drop=True)
    bank = []
    for _, r in rnd.iterrows():
        triples = build_row_kg(r, seed=seed)
        nm  = normalize_text(r["naseleno_mesto"])
        op  = normalize_text(r["opstina"])
        typ = normalize_text(r["tip_naseleno_mesto"])
        text = f"{nm} е {typ} и припаѓа на општина {op}."
        bank.append({
            "text": text,
            "kg_triples": triples,
            "kg_str": kg_to_nl_triples(triples),
            "sig": kg_signature(triples),
        })
    return bank

try:
    example_bank
except NameError:
    example_bank = fallback_example_bank_from_table(sample_df, n=700)

embedder = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

sig_texts = [e["sig"] for e in example_bank]
emb = embedder.encode(sig_texts, normalize_embeddings=True, show_progress_bar=True).astype("float32")

index = faiss.IndexFlatIP(emb.shape[1])
index.add(emb)

def retrieve_kshots(query_triples, k=4):
    qsig = kg_signature(query_triples)
    q = embedder.encode([qsig], normalize_embeddings=True).astype("float32")
    scores, ids = index.search(q, k)
    return [example_bank[i] for i in ids[0]]

def format_kshot_block(kshots):
    blocks = []
    for ex in kshots:
        blocks.append(
            "### Пример\n"
            "KG:\n" + ex["kg_str"] + "\n"
            "Текст:\n" + ex["text"]
        )
    return "\n\n".join(blocks)

from openai import OpenAI
import re, json

client = OpenAI(api_key=userdata.get("OPENAI_API_KEY"))
MODEL_ID = "gpt-4o-mini"

def call_llm(prompt, max_new_tokens=220, temperature=0.7, top_p=0.9):
    resp = client.chat.completions.create(
        model=MODEL_ID,
        messages=[{"role":"user","content":prompt}],
        max_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p
    )
    return resp.choices[0].message.content.strip()

def call_llm_json(prompt, max_new_tokens=420, temperature=0.2, top_p=0.9):
    prompt = prompt + "\n\nReturn ONLY valid JSON. No extra text."
    txt = call_llm(prompt, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p)
    m = re.search(r"\{.*\}", txt, flags=re.S)
    if not m:
        raise ValueError("No JSON found:\n" + txt[:400])
    return json.loads(m.group(0))

#test if the model works
print(call_llm("Кажи една реченица на македонски за Скопје.", max_new_tokens=60))

def style_directive(seed=None):
    variants = [
        "Енциклопедиски стил, кратко и јасно.",
        "Наративно, но формално; избегнувај претерување.",
        "Неутрален информативен опис, природен македонски.",
        "Краток опис со 2–3 реченици.",
        "Опис со 3–4 реченици, но без нови факти."
    ]
    if seed is not None:
        random.seed(seed)
    return random.choice(variants)

GEN_INSTRUCTION = (
    "Ти си автор кој пишува СТРОГО фактолошки текст на македонски.\n"
    "Ќе добиеш Knowledge Graph тројки. Твојот текст мора да ги содржи СИТЕ факти од тројките.\n"
    "Правила:\n"
    "- НЕ додавај нови факти/ентитети кои не се во тројките.\n"
    "- НЕ менувај релации (ако е 'припаѓа_на' мора да остане тоа значење).\n"
    "- Пиши природен македонски; без листи и без markdown.\n"
)

def build_generation_prompt(query_kg_str, kshots, seed=None):
    return (
        GEN_INSTRUCTION + "\n"
        + "Стил: " + style_directive(seed) + "\n\n"
        + format_kshot_block(kshots)
        + "\n\n### Влез (KG)\n" + query_kg_str
        + "\n\n### Излез\nТекст:"
    )

VOTE_INSTRUCTION = (
    "Избери го најдобриот кандидат според:\n"
    "1) Покриеност на сите тројки,\n"
    "2) Нема нови факти/ентитети,\n"
    "3) Кохерентен природен македонски.\n"
    "Одговори само со: A или B или C."
)

def build_vote_prompt(query_kg_str, cands):
    return (
        VOTE_INSTRUCTION
        + "\n\nKG:\n" + query_kg_str
        + "\n\nA:\n" + cands[0]
        + "\n\nB:\n" + cands[1]
        + "\n\nC:\n" + cands[2]
        + "\n\nОдговор:"
    )

ANNOT_INSTRUCTION = (
    "Ти си анототатор.\n"
    "Извлечи ентитети и релации од текстот.\n"
    "Ентитети: тип PER, LOC, ORG, TIME, NUM, MISC.\n"
    "Релации: користи само: е_тип, припаѓа_на, има_населено_место.\n"
    "Врати СТРОГО JSON со клучеви: entities, relations.\n"
    "entities: листа {text, type}\n"
    "relations: листа {head, relation, tail}\n"
)

def build_annot_prompt(text):
    return ANNOT_INSTRUCTION + "\n\nТекст:\n" + text + "\n\nJSON:"

#Coverage: fraction of triples whose head and tail are mentioned (fuzzy).

def triple_coverage(text: str, triples, threshold=88):
    t = normalize_text(text)
    ok = 0
    for h, r, o, ht, ot in triples:
        h_ok = fuzzy_in_text(h, t, threshold)
        o_ok = fuzzy_in_text(o, t, threshold)
        if h_ok and o_ok:
            ok += 1
    return ok / max(len(triples), 1)

def extract_candidate_entities(text: str):
    s = normalize_text(text)

    pattern = r"(?:[А-ШЃЌЉЊЏ][а-шѓќљњџ]+(?:\s+[А-ШЃЌЉЊЏ][а-шѓќљњџ]+)*)"
    ents = re.findall(pattern, s)

    ents = [e.strip() for e in ents if len(e.strip()) >= 3]
    uniq = []
    seen = set()
    for e in ents:
        k = norm_key(e)
        if k and k not in seen:
            uniq.append(e)
            seen.add(k)
    return uniq

def extract_quoted_entities(text: str) -> list[str]:
    return [s.strip() for s in re.findall(r"„([^“]+)“", text or "") if s.strip()]

def unknown_entity_ratio(text: str, kg_entities: list[str], match_threshold=90):
    cand_ents = extract_quoted_entities(text)
    if not cand_ents:
        return 0.0, []

    unknown = []
    for ce in cand_ents:
        m, score = best_fuzzy_match(ce, kg_entities, threshold=match_threshold)
        if m is None:
            unknown.append(ce)

    ratio = len(unknown) / max(len(cand_ents), 1)
    return ratio, unknown

def validate_candidate(text: str, triples, config=CONFIG):
    """
    Strict validation:
    - must cover all triples (or >= min_triple_coverage)
    - must not introduce too many unknown entities
    """
    if not text or len(text) < 15:
        return False, {"reason": "too_short"}

    cov = triple_coverage(text, triples, threshold=config["triple_match_threshold"])
    if cov < config["min_triple_coverage"]:
        return False, {"reason": "low_coverage", "coverage": cov}

    kg_ents = get_kg_entities(triples)
    unk_ratio, unk = unknown_entity_ratio(text, kg_ents, match_threshold=config["entity_match_threshold"])
    if unk_ratio > config["max_unknown_entity_ratio"]:
        return False, {"reason": "too_many_unknown_entities", "unknown_ratio": unk_ratio, "unknown": unk[:10]}

    return True, {"coverage": cov, "unknown_ratio": unk_ratio}

def generate_candidates(query_triples, kshots, seed=None, config=CONFIG):
    query_kg_str = kg_to_nl_triples(query_triples)
    prompt = build_generation_prompt(query_kg_str, kshots, seed=seed)
    cands = []
    for i in range(config["num_candidates"]):
        c = call_llm(
            prompt,
            max_new_tokens=config["gen_max_new_tokens"],
            temperature=config["gen_temperature"],
            top_p=0.9
        )
        cands.append(c)
    return cands

def deterministic_rank(text, triples, config=CONFIG):

    cov = triple_coverage(text, triples, threshold=config["triple_match_threshold"])
    unk_ratio, unk = unknown_entity_ratio(text, get_kg_entities(triples), match_threshold=config["entity_match_threshold"])
    length_penalty = 0.0
    if len(text) < 40: length_penalty += 0.05
    if len(text) > 420: length_penalty += 0.05
    score = cov - 0.7 * unk_ratio - length_penalty
    return score, {"coverage": cov, "unknown_ratio": unk_ratio, "unknown": unk[:8]}

def vote_best(query_triples, cands, config=CONFIG):
    """
    KGAST voting with deterministic tie-break.
    """
    query_kg_str = kg_to_nl_triples(query_triples)

    votes = {"A":0, "B":0, "C":0}
    vote_prompt = build_vote_prompt(query_kg_str, cands)
    for _ in range(config["vote_runs"]):
        v = call_llm(
            vote_prompt,
            max_new_tokens=config["vote_max_new_tokens"],
            temperature=config["vote_temperature"],
            top_p=1.0
        ).strip().upper()[:1]
        if v in votes:
            votes[v] += 1

    # pick by votes; if tie, deterministic ranking
    best_letter = max(votes, key=votes.get)
    best_idx = {"A":0, "B":1, "C":2}[best_letter]

    # tie-break
    sorted_votes = sorted(votes.items(), key=lambda x: x[1], reverse=True)
    if len(sorted_votes) >= 2 and sorted_votes[0][1] == sorted_votes[1][1]:
        ranks = []
        for i, c in enumerate(cands):
            s, meta = deterministic_rank(c, query_triples, config)
            ranks.append((s, i, meta))
        ranks.sort(reverse=True, key=lambda x: x[0])
        best_idx = ranks[0][1]
        best_letter = ["A","B","C"][best_idx]

    return cands[best_idx], votes, best_letter

def naive_filter_triples(triples, text, threshold=88):
    kept = []
    for h, r, o, ht, ot in triples:
        if fuzzy_in_text(h, text, threshold) and fuzzy_in_text(o, text, threshold):
            kept.append((h, r, o, ht, ot))
    return kept

def canonical_entity_key(ent_text, ent_type):
    return (norm_key(ent_text), ent_type)

def canonical_rel_key(h, r, o):
    return (norm_key(h), r, norm_key(o))

def majority_merge_annotations(ann_list, threshold=0.5):
    n = len(ann_list)
    ent_counts = {}
    rel_counts = {}

    for ann in ann_list:
        for e in ann.get("entities", []):
            key = canonical_entity_key(e.get("text",""), e.get("type","MISC"))
            if key[0]:
                ent_counts[key] = ent_counts.get(key, 0) + 1
        for rel in ann.get("relations", []):
            key = canonical_rel_key(rel.get("head",""), rel.get("relation",""), rel.get("tail",""))
            if key[0] and key[2] and key[1]:
                rel_counts[key] = rel_counts.get(key, 0) + 1

    ents = [{"text": k[0], "type": k[1]} for k,c in ent_counts.items() if c >= threshold*n]
    rels = [{"head": k[0], "relation": k[1], "tail": k[2]} for k,c in rel_counts.items() if c >= threshold*n]
    return {"entities": ents, "relations": rels}

def annotate_with_self_consistency(text, config=CONFIG):
    anns = []
    for _ in range(config["annot_runs"]):
        try:
            anns.append(call_llm_json(
                build_annot_prompt(text),
                max_new_tokens=config["json_max_new_tokens"],
                temperature=config["json_temperature"],
                top_p=0.9
            ))
        except Exception:
            continue
    if not anns:
        return {"entities": [], "relations": []}
    return majority_merge_annotations(anns, threshold=config["annot_threshold"])

"""
Snap extracted entities to the closest KG entity mention to reduce noise.
"""
def snap_entities_to_kg(entities, kg_entities, threshold=90):

    snapped = []
    for e in entities:
        txt = e.get("text","")
        typ = e.get("type","MISC")
        m, score = best_fuzzy_match(txt, kg_entities, threshold=threshold)
        if m is not None:
            snapped.append({"text": m, "type": typ})
    uniq = []
    seen = set()
    for e in snapped:
        k = (norm_key(e["text"]), e["type"])
        if k not in seen:
            uniq.append(e); seen.add(k)
    return uniq

def build_kg_entity_index(triples):
    idx = {}
    for h, r, o, ht, ot in triples:
        idx[str(h)] = ht or "LOC"
        idx[str(o)] = ot or "LOC"
    return idx

def entities_from_text_kg(triples, text: str, match_threshold=90):
    kg_idx = build_kg_entity_index(triples)
    kg_entities = list(kg_idx.keys())

    ents = []
    seen = set()
    for m in extract_quoted_entities(text):
        best, score = best_fuzzy_match(m, kg_entities, threshold=match_threshold)
        if best is None:
            continue
        if best in seen:
            continue
        seen.add(best)
        ents.append({
            "text": m,
            "kg_id": best,
            "type": kg_idx.get(best, "LOC"),
            "match_score": score
        })
    return ents

def kgast_synthesize_row(row, seed=None, config=CONFIG):
    triples = build_row_kg(row, seed=seed)
    kshots = retrieve_kshots(triples, k=config["kshots"])

    debug = {"regen_rounds": 0, "attempts": []}

    for round_i in range(config["max_regen_rounds"]):
        debug["regen_rounds"] = round_i + 1

        cands = generate_candidates(triples, kshots, seed=(None if seed is None else seed+round_i), config=config)

        # Coverage-first filter: keep only candidates that pass strict validation
        valids = []
        for c in cands:
            ok, meta = validate_candidate(c, triples, config)
            score, rank_meta = deterministic_rank(c, triples, config)
            debug["attempts"].append({"text": c, "valid": ok, "meta": {**meta, **rank_meta, "score": score}})
            if ok:
                valids.append(c)

        # If none valid, regen (next loop)
        if not valids:
            continue

        # If 1 valid, take it; else vote among first 3 (pad if fewer)
        if len(valids) == 1:
            best_text = valids[0]
            vote_stats = {"A":0,"B":0,"C":0}
            vote_pick = "A"
        else:
            # Ensure exactly 3 candidates for voting
            pool = valids[:3]
            while len(pool) < 3:
                pool.append(valids[-1])
            best_text, vote_stats, vote_pick = vote_best(triples, pool, config)

        # Final validation (must pass)
        ok, meta = validate_candidate(best_text, triples, config)
        if not ok:
            continue

        # Filter KG triples grounded in output text (robust fuzzy)
        filtered = naive_filter_triples(triples, best_text, threshold=config["triple_match_threshold"])

        kg_ents = get_kg_entities(triples)

        ann_entities = entities_from_text_kg(triples, best_text, match_threshold=config["entity_match_threshold"])


        filtered_relations = [{"head":h, "relation":r, "tail":o} for (h,r,o,_,_) in filtered]

        record = {
            "text": best_text,
            "kg_input": [{"head":h, "relation":r, "tail":o, "head_type":ht, "tail_type":ot} for (h,r,o,ht,ot) in triples],
            "kg_filtered": filtered_relations,
            "annotations": {
                "entities": ann_entities,
                "relations": filtered_relations
            },
            "meta": {
                "vote_stats": vote_stats,
                "vote_pick": vote_pick,
                "coverage": triple_coverage(best_text, triples, threshold=config["triple_match_threshold"]),
                "unknown_entity_ratio": unknown_entity_ratio(best_text, kg_ents, match_threshold=config["entity_match_threshold"])[0],
                "debug": debug if round_i == config["max_regen_rounds"]-1 else debug
            }
        }
        return record

    return {
        "text": "",
        "kg_input": [{"head":h, "relation":r, "tail":o, "head_type":ht, "tail_type":ot} for (h,r,o,ht,ot) in triples],
        "kg_filtered": [],
        "annotations": {"entities": [], "relations": []},
        "meta": {"failed": True, "debug": debug}
    }

N = min(200, len(sample_df))
rows_for_gen = sample_df.sample(n=N, random_state=42).reset_index(drop=True)

synthetic_records = []
for i, row in tqdm(rows_for_gen.iterrows(), total=len(rows_for_gen), desc="KGAST synth"):
    rec = kgast_synthesize_row(row, seed=1000+i, config=CONFIG)
    if rec.get("meta", {}).get("failed"):
        continue
    synthetic_records.append(rec)

def sft_prompt_from_kg(rec):
    kg_lines = []
    for t in rec["kg_input"]:
        kg_lines.append(f'("{t["head"]}", "{t["relation"]}", "{t["tail"]}")')
    kg_block = "\n".join(kg_lines)
    return (
        "Напиши 2–4 кохерентни реченици на македонски кои ги вклучуваат сите факти од тројките.\n"
        "Не додавај нови факти или ентитети.\n\n"
        "Тројки:\n" + kg_block + "\n\nТекст:"
    )

out_ie = "mk_kgast_ie.jsonl"
with open(out_ie, "w", encoding="utf-8") as f:
    for r in synthetic_records:
        if r.get("text"):
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

out_sft = "mk_kgast_sft.jsonl"
with open(out_sft, "w", encoding="utf-8") as f:
    for r in synthetic_records:
        if r.get("text"):
            f.write(json.dumps({
                "prompt": sft_prompt_from_kg(r),
                "completion": r["text"],
                "meta": r["meta"]
            }, ensure_ascii=False) + "\n")

out_ie, out_sft

import json
with open("mk_kgast_sft.jsonl", "r", encoding="utf-8") as f:
    first = json.loads(f.readline())
first.keys(), first

with open("mk_kgast_ie.jsonl", "r", encoding="utf-8") as f:
    first = json.loads(f.readline())
first.keys(), first

import json, re
import pandas as pd
import matplotlib.pyplot as plt

SFT_PATH = "mk_kgast_sft.jsonl"
IE_PATH  = "mk_kgast_ie.jsonl"

def read_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                rows.append(json.loads(line))
    return rows

def extract_quoted(text: str):
    return [s.strip() for s in re.findall(r"„([^“]+)“", text or "")]

def collect_kg_entities(rec):
    ents = set()
    kg = rec.get("kg_input") or []
    for t in kg:
        if isinstance(t, (list, tuple)) and len(t) >= 3:
            ents.add(str(t[0])); ents.add(str(t[2]))
        elif isinstance(t, dict):
            h = t.get("head") or t.get("subject")
            o = t.get("tail") or t.get("object")
            if h is not None: ents.add(str(h))
            if o is not None: ents.add(str(o))
    return ents

sft = read_jsonl(SFT_PATH)
ie  = read_jsonl(IE_PATH)


sft_df = pd.DataFrame([{
    "text": (rec.get("completion") or rec.get("text") or "").strip(),
    "coverage": (rec.get("meta", {}) or {}).get("coverage"),
    "unknown_entity_ratio": (rec.get("meta", {}) or {}).get("unknown_entity_ratio"),
} for rec in sft])
sft_df["text_words"] = sft_df["text"].str.split().str.len().fillna(0).astype(int)

ie_rows = []
for rec in ie:
    text = (rec.get("text") or rec.get("completion") or "").strip()
    meta = rec.get("meta", {}) or {}
    ann  = rec.get("annotations", {}) or {}
    kg_ents = collect_kg_entities(rec)
    quoted = extract_quoted(text)
    grounded = [q for q in quoted if q in kg_ents]
    ie_rows.append({
        "text": text,
        "coverage": meta.get("coverage"),
        "unknown_entity_ratio": meta.get("unknown_entity_ratio"),
        "text_words": len(text.split()) if text else 0,
        "quoted_mentions_in_kg": len(grounded),
        "entities_annotated": len(ann.get("entities") or []),
        "relations_annotated": len(ann.get("relations") or []),
    })
ie_df = pd.DataFrame(ie_rows)

# --- Evaluation 1: summary ---
def summarize(name, df):
    cov = pd.to_numeric(df["coverage"], errors="coerce")
    unk = pd.to_numeric(df["unknown_entity_ratio"], errors="coerce")
    print(f"\n=== {name} summary ===")
    print("records:", len(df))
    print("avg words:", float(df["text_words"].mean()))
    if cov.notna().any():
        print("coverage mean/median:", float(cov.mean()), float(cov.median()))
    if unk.notna().any():
        print("unknown ratio mean/median:", float(unk.mean()), float(unk.median()))
    if "entities_annotated" in df.columns:
        print("avg entities:", float(df["entities_annotated"].mean()))
        print("avg relations:", float(df["relations_annotated"].mean()))
        print("avg grounded mentions:", float(df["quoted_mentions_in_kg"].mean()))

summarize("SFT", sft_df)
summarize("IE", ie_df)

